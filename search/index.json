[{"content":" 题目链接：1. 两数之和 知识回顾：哈希表 暴力解法 使用两层 for 循环遍历所有两数组合 时间复杂度：$O (n^2)$ 空间复杂度：$O (1)$ 其中 $n$ 为数组长度 哈希表优化 核心思想：将问题 num1 + num2 = target 转化为 num2 = target - num1 若能快速判断 target - num1 是否已出现过，并获取其索引，则可一次遍历解决 在普通列表中查找元素需 $O(n)$，但哈希表支持平均 $O (1)$ 的查找 算法步骤： 初始化空哈希表 hash_map 遍历数组，对每个元素 num 及其索引 i： 若 target - num 已在哈希表中，返回 [hash_map[target - num], i] 否则，将 num: i 存入哈希表 复杂度分析 时间复杂度：$O(n)$ 空间复杂度：$O(n)$ 1 2 3 4 5 6 7 8 class Solution: def twoSum(self, nums: List[int], target: int) -\u0026gt; List[int]: hash_map = {} for i, num in enumerate(nums): complement = target - num if complement in hash_map: return [hash_map[complement], i] hash_map[num] = i ","date":"2025-12-17T00:00:00Z","permalink":"https://example.com/p/001-two-sum/","title":"01.两数之和"},{"content":" 题目连接： 49. 字母异位词分组 知识回顾： 哈希表 Python 中 defaultdict 的使用 关键观察：对字符串排序后，所有字母异位词会变成相同的字符串。 因此，我们可以将排序后的字符串作为哈希表的键，原始字符串作为值存入对应的列表中。 算法步骤： 初始化空哈希表 遍历字符串数组 将字符串进行排序 sorted_str 若 sorted_str 在哈希表中，则新增字母异位词 否侧，将 sorted_str: [srt] 存入哈希表 复杂度分析 时间复杂度：$O(nm\\log m)$ 其中 $n$ 为列表长度 $m$ 为字符串长度 长度为 $m$ 的字符串排序耗时 $m\\log m$ 空间复杂度： $O(nm)$ $n$ 为字符串个数，$m$ 为字符串平均长度 1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution: def groupAnagrams(self, strs: List[str]) -\u0026gt; List[List[str]]: # 将字符串重新排序，作为字典的键 # 字典的值用于存储字符串 my_dict = {} for str_ in strs: sorted_str = \u0026#34;\u0026#34;.join(sorted(str_)) if sorted_str in my_dict: my_dict[sorted_str].append(str_) else: my_dict[sorted_str] = [str_] return list(my_dict.values()) 使用 defaultdict(list) 简化逻辑 在普通字典（dict）中，当你访问一个不存在的键时，会抛出 KeyError。因此，在向字典中追加值之前，必须先判断该键是否存在： 1 2 3 4 if key in my_dict: my_dict[key].append(value) else: my_dict[key] = [value] 而 collections.defaultdict 是 dict 的子类，它在访问不存在的键时，会自动调用传入的工厂函数（如 list）来生成默认值。 当你写 defaultdict(list) 时，意味着： “如果某个键不存在，就自动为它创建一个空列表 []。” 因此，你可以直接写： 1 hashmap[key].append(value) 即使 key 之前从未出现过，hashmap[key] 也会返回一个空列表，append 操作安全有效。 1 2 from collections import defaultdict hashmap = defaultdict(list) ","date":"2025-12-17T00:00:00Z","permalink":"https://example.com/p/049-group-anagrams/","title":"049.字母异位词分组"},{"content":" 未完持续 打算发发\n","date":"2025-12-17T00:00:00Z","permalink":"https://example.com/p/hash-table/","title":"哈希表"},{"content":"一、PINN 简介 Physics-Informed Neural Networks 1.1 什么是 PINN 背景 很多科学里的问题，没有太多数据可以学，但我们有物理规律 比如 海水怎么流（数据少，但物理规律明确） 心脏里的血流怎么走（不好测，但物理知道） 地震波怎样传播（难观察，但方程知道） 普通 AI 不懂这些规律，会乱猜 PINN 就能利用物理规律，少量数据也能算得非常准 用一句话来说，PINN 就是：让“神经网络”按照“物理定律”来学习的一种方法 这里给一个比喻 训练一个普通的 AI 模型，就像教一个小孩画画。这个孩子只知道照着例子画，你给什么他画什么（照猫画虎），没道理可讲（即神经网络输出的结果只与真实标签做损失，监督学习） 但是 PINN 不一样 给它一点点实际数据，它能学 最重要的是，它还“从小背过物理书”，知道 水怎么流 热怎么传 风怎么吹 东西怎么震动 光怎么折射 这些都叫 “物理方程” 所以它不是瞎学，它有“规矩”，有“脑子”，有“常识” 就像绘画高手，懂结构、光影、透视，所以会画的更好 1.2 PINN 与普通神经网络的区别 神经网络用来“猜”答案 比如猜温度、压力、水的速度…… 但是它的猜法必须满足“物理方程” 比如能量守恒、质量守恒、牛顿定律…… 就像告诉它：你不能乱猜，要合规矩！ 用“数据 + 物理”一起纠正它 它猜得不对，马上被纠正 物理方程也不让它瞎走 于是越猜越准 举例子 例子 1：屋子里温度怎么变？ 假设房间里有暖气，问 30 分钟后哪个角落最暖？ 普通神经网络： 要给它很多温度计数据，它才能学。 PINN： 给它少量数据 + 热传导定律，它自己能算出整个房间的温度分布。 例子 2：水从高处往低处流 PINN 知道“水往低处流”的规律，不需要测每个点的水速 例子 3：心脏里的血流 医院无法把心脏每个点都装传感器 但 PINN 用少量 MRI 数据 + 流体力学规律 就能推算整个心脏内部血流 二、PINN 的框架 PINN 模型通常由一个深度神经网络构成，其特点在于损失函数中加入了物理信息项，即所遵循的物理定律 PINN 主要分以下两个部分 神经网络结构 网络输入通常是问题域中的位置、时间等参数，输出是感兴趣物理量的估计值（例如速度、压力等） 定义损失函数 损失函数是模型训练中的关键部分，又包含以下两部分 数据损失项 这部分用来衡量网络预测输出与实际观测数据之间的差异，目的是使网络能够尽可能拟合数据 物理损失项 这部分是 PINN 独有的，它考量了网络预测结果是否满足物理定律 将网络预测的物理量代入相应的物理定律（通常是微分方程）中计算得到的残差构成这一部分损失函数，从而确保了物理一致性 三、PINN 的应用 这里以浅水方程为例：构建一个能够利用地形深度输入、并通过物理方程约束来预测浅水动力过程（如流速、水深演化）的 PINN 模型 基于地形的洪水模拟 浅水方程（Shallow Water Equations, SWE）二维形式如下（以保守型为例，假设地形平坦，无摩擦） 质量守恒、动量守恒 其中 $h(x,y,t)$：水深 $u,v$：水平速度 $g$：重力加速度 浅水方程的核心目标是描述 水深如何随时间变化：（质量守恒，Continuity） 水流速度如何随时间变化：（动量守恒，Momentum） 换句话说 浅水方程告诉我们：水有多深，以及它往哪里、以多快的速度流动。 由于忽略了垂向加速度并假设压力近似为静水压力，浅水方程比完整的 Navier–Stokes 方程更简单，但仍保留了大部分重要物理特征，非常适合大尺度水动力模拟 神经网络预测 $(h_θ​,u_θ​,v_θ​)$ 将其代入 PDE，构造残差 物理损失 最终损失 伪代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 伪代码概念演示 for batch in dataloader: img_obs, DEM, cond = batch # 遥感观测，地形等 t = sample_time_steps() x_t = q_sample(img_obs, t) # 扰动成噪声 (扩散训练常规) eps_pred = model(x_t, t, cond) # 去噪网络输出噪声估计 # 基本扩散损失 L_data = mse(eps_pred, true_eps) # decode to physical fields (e.g., h,u,v) from x_t or eps_pred h, u, v = decode_physical_field(x_t, eps_pred, DEM, cond) # compute physics residuals (using finite diff or differentiable solver) residual = shallow_water_residual(h, u, v, DEM) L_phy = mse(residual, 0) loss = L_data + lambda_phy * L_phy loss.backward() optimizer.step() 参考文献 PINN——加入物理约束的神经网络 - 知乎 物理信息神经网络（PINN）: 将物理知识融合到深度学习中 Physics Informed Neural Networks (PINNs) [Physics Informed Machine Learning] PINN核心技术揭秘：如何用神经网络求解偏微分方程？ ","date":"2025-12-01T00:00:00Z","permalink":"https://example.com/p/pinn-physics-informed-neural-networks/","title":"PINN——物理信息神经网络"}]